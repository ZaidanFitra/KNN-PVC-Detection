{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36bef0d6",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9150058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_dir = './mit-bih-arrhythmia-database-1.0.0/'\n",
    "\n",
    "records = [f.split('.')[0] for f in os.listdir(data_dir) if f.endswith('.dat')]\n",
    "\n",
    "sr = 360  # Sampling rate\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for record_name in records:\n",
    "    print(f\"Memproses record {record_name}...\")\n",
    "    \n",
    "    try:\n",
    "        record = wfdb.rdrecord(os.path.join(data_dir, record_name))\n",
    "        ann = wfdb.rdann(os.path.join(data_dir, record_name), 'atr')\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal memuat record {record_name}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    ecg = record.p_signal[:, 0]  \n",
    "    rpeaks = ann.sample  \n",
    "    symbols = ann.symbol \n",
    "    \n",
    "    valid_indices = [i for i, sym in enumerate(symbols) if sym in ['N', 'V']]\n",
    "    rpeaks = rpeaks[valid_indices]\n",
    "    symbols = [symbols[i] for i in valid_indices]\n",
    "    \n",
    "    if len(rpeaks) == 0:\n",
    "        print(f\"Tidak ada beat 'N' atau 'V' di record {record_name}. Lewati.\")\n",
    "        continue\n",
    "    \n",
    "    cleaned = nk.ecg_clean(ecg, sampling_rate=sr, method='neurokit')\n",
    "    \n",
    "    try:\n",
    "        _, info = nk.ecg_delineate(cleaned, rpeaks, sampling_rate=sr, method='peak')\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal mendelineasi record {record_name}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Ekstraksi fitur\n",
    "    for i, rpeak in enumerate(rpeaks):\n",
    "        # Interval RR sebelum\n",
    "        rr_prev = (rpeak - rpeaks[i-1]) / sr if i > 0 else np.nan\n",
    "        \n",
    "        # Interval RR sesudah\n",
    "        rr_next = (rpeaks[i+1] - rpeak) / sr if i < len(rpeaks)-1 else np.nan\n",
    "        \n",
    "        # Durasi QRS\n",
    "        q_peak = info.get('ECG_Q_Peaks', [])[i] if i < len(info.get('ECG_Q_Peaks', [])) else np.nan\n",
    "        s_peak = info.get('ECG_S_Peaks', [])[i] if i < len(info.get('ECG_S_Peaks', [])) else np.nan\n",
    "        qrs_duration = (s_peak - q_peak) / sr if not np.isnan(q_peak) and not np.isnan(s_peak) else np.nan\n",
    "        \n",
    "        features = [rr_prev, rr_next, qrs_duration]\n",
    "        label = 1 if symbols[i] == 'V' else 0 \n",
    "        all_features.append(features)\n",
    "        all_labels.append(label)\n",
    "\n",
    "all_features = np.array(all_features)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "for i in range(all_features.shape[1]):\n",
    "    col = all_features[:, i]\n",
    "    if np.any(np.isnan(col)):\n",
    "        mean_val = np.nanmean(col)\n",
    "        col[np.isnan(col)] = mean_val\n",
    "        all_features[:, i] = col\n",
    "\n",
    "np.save('features.npy', all_features)\n",
    "np.save('labels.npy', all_labels)\n",
    "\n",
    "df = pd.DataFrame(all_features, columns=['RR_Prev', 'RR_Next', 'QRS_Duration'])\n",
    "df['Label'] = all_labels\n",
    "df.to_csv('mitbih_features.csv', index=False)\n",
    "\n",
    "print(\"Ekstraksi fitur selesai.\")\n",
    "print(f\"Total beat yang diproses: {len(all_labels)}\")\n",
    "print(f\"Jumlah beat Normal (0): {np.sum(all_labels == 0)}\")\n",
    "print(f\"Jumlah beat PVC (1): {np.sum(all_labels == 1)}\")\n",
    "print(\"Data disimpan ke 'features.npy', 'labels.npy', dan 'mitbih_features.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1619b2d4",
   "metadata": {},
   "source": [
    "Data Balancing and Features Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e82fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "try:\n",
    "    features = np.load('features.npy')\n",
    "    labels = np.load('labels.npy')\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'features.npy' atau 'labels.npy' tidak ditemukan. Pastikan telah menjalankan kode ekstraksi fitur sebelumnya.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Distribusi data sebelum penyeimbangan:\")\n",
    "print(f\"Jumlah beat Normal (0): {np.sum(labels == 0)}\")\n",
    "print(f\"Jumlah beat PVC (1): {np.sum(labels == 1)}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Terapkan SMOTE untuk seimbangkan data\n",
    "smote = SMOTE(random_state=42)\n",
    "balanced_features, balanced_labels = smote.fit_resample(features_scaled, labels)\n",
    "\n",
    "print(\"\\nDistribusi data setelah penyeimbangan:\")\n",
    "print(f\"Jumlah beat Normal (0): {np.sum(balanced_labels == 0)}\")\n",
    "print(f\"Jumlah beat PVC (1): {np.sum(balanced_labels == 1)}\")\n",
    "\n",
    "np.save('balanced_features.npy', balanced_features)\n",
    "np.save('balanced_labels.npy', balanced_labels)\n",
    "\n",
    "df_balanced = pd.DataFrame(balanced_features, columns=['RR_Prev', 'RR_Next', 'QRS_Duration'])\n",
    "df_balanced['Label'] = balanced_labels\n",
    "df_balanced.to_csv('mitbih_balanced_features.csv', index=False)\n",
    "\n",
    "print(\"\\nData seimbang disimpan ke 'balanced_features.npy', 'balanced_labels.npy', dan 'mitbih_balanced_features.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0254b3f",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe491d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "try:\n",
    "    features = np.load('balanced_features.npy')\n",
    "    labels = np.load('balanced_labels.npy')\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'balanced_features.npy' atau 'balanced_labels.npy' tidak ditemukan. Pastikan telah menjalankan kode penyeimbangan data sebelumnya.\")\n",
    "    exit()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Lakukan GridSearchCV\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nParameter terbaik:\", grid_search.best_params_)\n",
    "print(\"Skor F1 terbaik (cross-validation):\", grid_search.best_score_)\n",
    "\n",
    "best_knn = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "print(\"\\nAkurasi pada data pengujian:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nLaporan Klasifikasi:\\n\", classification_report(y_test, y_pred, target_names=['Normal', 'PVC']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'PVC'], yticklabels=['Normal', 'PVC'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Prediksi')\n",
    "plt.ylabel('Aktual')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "joblib.dump(best_knn, 'knn_model.pkl')\n",
    "print(\"\\nModel terbaik disimpan ke 'knn_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cfed3c",
   "metadata": {},
   "source": [
    "Export Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "try:\n",
    "    features = np.load('features.npy')\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'features.npy' tidak ditemukan. Pastikan file ada.\")\n",
    "    exit()\n",
    "\n",
    "# Fit StandardScaler pada data pelatihan asli\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features)\n",
    "\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"Scaler disimpan ke 'scaler.pkl'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
